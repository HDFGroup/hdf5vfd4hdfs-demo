<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2018-10-25 Thu 10:29 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>HDF5 Virtual File Driver for HDFS - Demonstration</title>
<meta name="generator" content="Org mode" />
<meta name="author" content="Gerd Heber" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>

    <link href='http://fonts.googleapis.com/css?family=Source+Sans+Pro:400,700,400italic,700italic&subset=latin,latin-ext' rel='stylesheet' type='text/css'>
    <link href='http://fonts.googleapis.com/css?family=Source+Code+Pro:400,700' rel='stylesheet' type='text/css'>
    <style type='text/css'>
       body {
          font-family: 'Source Sans Pro', sans-serif;
       }
       pre, code {
          font-family: 'Source Code Pro', monospace;
       }
    </style>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2018 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
</head>
<body>
<div id="content">
<h1 class="title">HDF5 Virtual File Driver for HDFS - Demonstration</h1>
<p>
This demonstration will show you how to:
</p>

<ol class="org-ol">
<li>Copy an HDF5 file into an HDFS file system</li>
<li>List the file structure by running <code>h5ls</code></li>
<li>Print detailed information by running <code>h5dump</code></li>
<li>Extract data from a dataset with <code>h5dump</code></li>
<li>Use Hadoop Streaming to collect data from multiple HDF5 files</li>
</ol>

<p>
Before you proceed, please review the following prerequisites.
</p>

<div id="outline-container-org853556a" class="outline-2">
<h2 id="org853556a">Prerequisites</h2>
<div class="outline-text-2" id="text-org853556a">
<ul class="org-ul">
<li>A version of the HDF5 library (HDF5 1.10.4 or later) with the HDFS Virtual
File Driver (VFD) enabled</li>
<li>A compatible version of Hadoop (e.g., 3.1.1)</li>
<li>An HDFS file system (a cluster or local installation)</li>
</ul>

<p>
With these prerequisites in place, make sure that your environment is
configured for the dependencies (Java, Hadoop).
</p>
</div>
</div>


<div id="outline-container-org9eb45fb" class="outline-2">
<h2 id="org9eb45fb">Environment</h2>
<div class="outline-text-2" id="text-org9eb45fb">
<p>
The HDF5 VFD for HDFS, whether used in your own application or with
HDF5 command line tools, depends on certain environment variables and
shared libraries.
</p>

<p>
Please ensure that <code>JAVA_HOME</code>, <code>HADOOP_HOME</code>, and <code>CLASSPATH</code> reflect your
environment. An example is provided below:
</p>

<div class="org-src-container">
<pre class="src src-shell"><span style="color: #006FE0; font-weight: bold;">export</span> <span style="color: #BA36A5;">JAVA_HOME</span>=/usr/lib/jvm/java-openjdk
<span style="color: #006FE0; font-weight: bold;">export</span> <span style="color: #BA36A5;">HADOOP_HOME</span>=$<span style="color: #BA36A5;">HOME</span>/work/hadoop-3.1.1
<span style="color: #006FE0; font-weight: bold;">export</span> <span style="color: #BA36A5;">CLASSPATH</span>=<span style="color: #FF1493;">`$HADOOP_HOME/bin/hadoop classpath --glob`</span>
</pre>
</div>

<p>
The HDFS VFD depends on the following shared libraries: <code>libhdf5.so</code>,
<code>libhdfs.so</code>, <code>libjvm.so</code>. Please locate these dependencies and configure
the <code>LD_LIBRARY_PATH</code> accordingly. An example is shown below:
</p>

<div class="org-src-container">
<pre class="src src-shell"><span style="color: #006FE0; font-weight: bold;">export</span> <span style="color: #BA36A5;">LD_LIBRARY_PATH</span>=$<span style="color: #BA36A5;">JAVA_HOME</span>/jre/lib/amd64/server:$<span style="color: #BA36A5;">LD_LIBRARY_PATH</span>
<span style="color: #006FE0; font-weight: bold;">export</span> <span style="color: #BA36A5;">LD_LIBRARY_PATH</span>=$<span style="color: #BA36A5;">HADOOP_HOME</span>/lib/native:$<span style="color: #BA36A5;">LD_LIBRARY_PATH</span>
<span style="color: #006FE0; font-weight: bold;">export</span> <span style="color: #BA36A5;">LD_LIBRARY_PATH</span>=$<span style="color: #BA36A5;">HOME</span>/.local/lib:$<span style="color: #BA36A5;">LD_LIBRARY_PATH</span>
</pre>
</div>

<p>
For convenience, we add the directories containing the Hadoop and HDF5
command line tools to our <code>PATH</code> environment variable.
</p>

<div class="org-src-container">
<pre class="src src-shell"><span style="color: #006FE0; font-weight: bold;">export</span> <span style="color: #BA36A5;">PATH</span>=$<span style="color: #BA36A5;">HADOOP_HOME</span>/bin:$<span style="color: #BA36A5;">HOME</span>/.local/bin:$<span style="color: #BA36A5;">PATH</span>
</pre>
</div>

<p>
To get started you need to know the host name and port of the HDFS namenode.
In the examples below, the host name is <code>jelly.ad.hdfgroup,org</code> and the port
number is 8020. Verify the availability of an HDFS file system via the
<code>hdfs dfs -ls</code> command:
</p>

<div class="org-src-container">
<pre class="src src-shell">hdfs dfs -ls hdfs://jelly.ad.hdfgroup.org:8020/
</pre>
</div>

<pre class="example">
Found 5 items
drwxrwxrwx   - hdfs  supergroup          0 2016-04-05 21:26 hdfs://jelly.ad.hdfgroup.org:8020/benchmarks
drwxr-xr-x   - hbase supergroup          0 2016-04-05 21:26 hdfs://jelly.ad.hdfgroup.org:8020/hbase
drwxrwxrwt   - hdfs  supergroup          0 2018-10-25 09:55 hdfs://jelly.ad.hdfgroup.org:8020/tmp
drwxr-xr-x   - hdfs  supergroup          0 2018-10-10 16:14 hdfs://jelly.ad.hdfgroup.org:8020/user
drwxr-xr-x   - hdfs  supergroup          0 2016-04-05 21:27 hdfs://jelly.ad.hdfgroup.org:8020/var

</pre>
</div>
</div>


<div id="outline-container-org1024166" class="outline-2">
<h2 id="org1024166">Demonstration</h2>
<div class="outline-text-2" id="text-org1024166">
</div>
<div id="outline-container-orgaae9095" class="outline-3">
<h3 id="orgaae9095">Copying an HDF5 file to HDFS</h3>
<div class="outline-text-3" id="text-orgaae9095">
<p>
If you have an existing HDF5 or NetCDF-4 file, you can skip this step.
If you don't have an HDF5 file, it's easy to create one via <code>h5mkgrp</code>:
</p>

<div class="org-src-container">
<pre class="src src-shell">h5mkgrp -p sample.h5 /A/few/groups
ls -al sample.h5
</pre>
</div>

<pre class="example">

-rw-r--r--. 1 gheber hdf 3896 Oct 25 10:22 sample.h5

</pre>

<p>
Copy your example file or <code>sample.h5</code> to HDFS as follows:
</p>

<div class="org-src-container">
<pre class="src src-shell">hdfs dfs -copyFromLocal sample.h5 hdfs://jelly.ad.hdfgroup.org/tmp
hdfs dfs -ls hdfs://jelly.ad.hdfgroup.org/tmp/sample.h5
</pre>
</div>

<pre class="example">

-rw-r--r--   3 gheber supergroup       3896 2018-10-25 10:23 hdfs://jelly.ad.hdfgroup.org/tmp/sample.h5

</pre>

<p>
If you do not have the permission to write to HDFS' <code>/tmp</code> directory, you
will see an error message and must choose a target directory, such as
your home directory (in HDFS), where you have write permission.
</p>
</div>
</div>


<div id="outline-container-org31ebd89" class="outline-3">
<h3 id="org31ebd89">Listing the HDF5 file structure via <code>h5ls</code></h3>
<div class="outline-text-3" id="text-org31ebd89">
<p>
The <code>h5ls</code> command line tool lists information about objects in an
HDF5 file. There is no difference in the behavior of <code>h5ls</code> between listing
information about objects in an HDF5 file that is stored in a local
file system vs. HDFS. There currently one additional required argument,
<code>--vfd=hdfs</code> to tell <code>h5ls</code> to use the HDFS VFD instead of the default POSIX
VFD. Unless the HDFS file system is running on <code>localhost</code>, it is also
necessary to specify the host name and port number of the HDFS namenode
via the <code>--hdfs-attrs</code> option.
</p>

<div class="org-src-container">
<pre class="src src-shell">h5ls --vfd=hdfs --hdfs-attrs=<span style="color: #008000;">"(hdfs://jelly.ad.hdfgroup.org,8020,,,)"</span> <span style="color: #008000;">\</span>
     -r /tmp/sample.h5
</pre>
</div>

<pre class="example">

/                        Group
/A                       Group
/A/few                   Group
/A/few/groups            Group

</pre>

<p>
A slightly more complex example is shown below:
</p>

<div class="org-src-container">
<pre class="src src-shell">h5ls --vfd=hdfs --hdfs-attrs=<span style="color: #008000;">"(hdfs://jelly.ad.hdfgroup.org,8020,,,)"</span> <span style="color: #008000;">\</span>
     -r /tmp/efitOut.nc | head -n 50
</pre>
</div>

<pre class="example">

/                        Group
/coords                  Type
/equilibriumStatus       Dataset {22845}
/equilibriumStatusInteger Dataset {22845}
/equilibriumStatusType   Type
/errorMessageDim         Dataset {20}
/errorMessages           Dataset {22845, 20}
/input                   Group
/input/bVacRadiusProduct Group
/input/bVacRadiusProduct/values Dataset {22845}
/input/codeControls      Group
/input/codeControls/alpgamSwitch Dataset {1}
/input/codeControls/computeChi2WithWeights Dataset {1}
/input/codeControls/computeConstraintsWithDz Dataset {1}
/input/codeControls/fcurrtFit Dataset {1}
/input/codeControls/fitDzAlgorithm Dataset {1}
/input/codeControls/lcfsTol Dataset {1}
/input/constraints       Group
/input/constraints/diamagneticFlux Group
/input/constraints/diamagneticFlux/computed Dataset {22845}
/input/constraints/diamagneticFlux/sigma Dataset {22845}
/input/constraints/diamagneticFlux/target Dataset {22845}
/input/constraints/diamagneticFlux/weights Dataset {22845}
/input/constraints/fluxLoops Group
/input/constraints/fluxLoops/computed Dataset {22845, 36}
/input/constraints/fluxLoops/fluxLoopDim Dataset {36}
/input/constraints/fluxLoops/fluxLoopElementDim Dataset {2}
/input/constraints/fluxLoops/id Dataset {36}
/input/constraints/fluxLoops/rValues Dataset {36, 2}
/input/constraints/fluxLoops/sigmas Dataset {22845, 36}
/input/constraints/fluxLoops/target Dataset {22845, 36}
/input/constraints/fluxLoops/toroidalAngleBegin Dataset {36, 2}
/input/constraints/fluxLoops/toroidalAngleEnd Dataset {36, 2}
/input/constraints/fluxLoops/weights Dataset {22845, 36}
/input/constraints/fluxLoops/zValues Dataset {36, 2}
/input/constraints/ironModel Group
/input/constraints/ironModel/absoluteError Dataset {1}
/input/constraints/ironModel/fittingAbsoluteError Dataset {1}
/input/constraints/ironModel/fittingRelativeError Dataset {1}
/input/constraints/ironModel/geometry Group
/input/constraints/ironModel/geometry/Dang Dataset {1, 96}
/input/constraints/ironModel/geometry/Dang2 Dataset {1, 96}
/input/constraints/ironModel/geometry/Eang Dataset {1, 96}
/input/constraints/ironModel/geometry/Eang2 Dataset {1, 96}
/input/constraints/ironModel/geometry/boundaryCoordsR Dataset {1, 96}
/input/constraints/ironModel/geometry/boundaryCoordsZ Dataset {1, 96}
/input/constraints/ironModel/geometry/boundaryIntervalCount Dataset {1}
/input/constraints/ironModel/geometry/boundaryLength Dataset {1}
/input/constraints/ironModel/geometry/boundaryNodeCount Dataset {1}
/input/constraints/ironModel/geometry/boundaryNodeDim Dataset {96}
</pre>
</div>
</div>


<div id="outline-container-org45708df" class="outline-3">
<h3 id="org45708df">Printing detailed information with <code>h5dump</code></h3>
<div class="outline-text-3" id="text-org45708df">
<p>
The <code>h5dump</code> command line tool lists detailed information about objects in an
HDF5 file. There is no difference in the behavior of <code>h5dump</code> between listing
information about objects in an HDF5 file that is stored in a local
file system vs. HDFS. There currently one additional required argument,
<code>--filedriver=hdfs</code> to tell <code>h5dump</code> to use the HDFS VFD instead of the
default POSIX VFD. Unless the HDFS file system is running on <code>localhost</code>, it
is also necessary to specify the host name and port number of the HDFS
namenode.
</p>

<div class="org-src-container">
<pre class="src src-shell">h5dump --filedriver=hdfs <span style="color: #008000;">\</span>
       --hdfs-attrs=<span style="color: #008000;">"(hdfs://jelly.ad.hdfgroup.org,8020,,,)"</span> <span style="color: #008000;">\</span>
       -pB <span style="color: #008000;">\</span>
       /tmp/sample.h5
</pre>
</div>

<pre class="example">

&gt; &gt; HDF5 "/tmp/sample.h5" {
SUPER_BLOCK {
   SUPERBLOCK_VERSION 0
   FREELIST_VERSION 0
   SYMBOLTABLE_VERSION 0
   OBJECTHEADER_VERSION 0
   OFFSET_SIZE 8
   LENGTH_SIZE 8
   BTREE_RANK 16
   BTREE_LEAF 4
   ISTORE_K 32
   FILE_SPACE_STRATEGY H5F_FSPACE_STRATEGY_FSM_AGGR
   FREE_SPACE_PERSIST FALSE
   FREE_SPACE_SECTION_THRESHOLD 1
   FILE_SPACE_PAGE_SIZE 4096
   USER_BLOCK {
      USERBLOCK_SIZE 0
   }
}
GROUP "/" {
   GROUP "A" {
      GROUP "few" {
         GROUP "groups" {
         }
      }
   }
}
}
</pre>

<p>
A slightly more complex example is shown below:
</p>

<div class="org-src-container">
<pre class="src src-shell">h5dump --filedriver=hdfs <span style="color: #008000;">\</span>
       --hdfs-attrs=<span style="color: #008000;">"(hdfs://jelly.ad.hdfgroup.org,8020,,,)"</span> <span style="color: #008000;">\</span>
       -pBH /tmp/efitOut.nc | head -n 50
</pre>
</div>

<pre class="example">

&gt; HDF5 "/tmp/efitOut.nc" {
SUPER_BLOCK {
   SUPERBLOCK_VERSION 2
   FREELIST_VERSION 0
   SYMBOLTABLE_VERSION 0
   OBJECTHEADER_VERSION 0
   OFFSET_SIZE 8
   LENGTH_SIZE 8
   BTREE_RANK 16
   BTREE_LEAF 4
   ISTORE_K 32
   FILE_SPACE_STRATEGY H5F_FSPACE_STRATEGY_FSM_AGGR
   FREE_SPACE_PERSIST FALSE
   FREE_SPACE_SECTION_THRESHOLD 1
   FILE_SPACE_PAGE_SIZE 4096
   USER_BLOCK {
      USERBLOCK_SIZE 0
   }
}
GROUP "/" {
   ATTRIBUTE "Conventions" {
      DATATYPE  H5T_STRING {
         STRSIZE 20;
         STRPAD H5T_STR_NULLTERM;
         CSET H5T_CSET_ASCII;
         CTYPE H5T_C_S1;
      }
      DATASPACE  SCALAR
   }
   ATTRIBUTE "codeVersion" {
      DATATYPE  H5T_STRING {
         STRSIZE 11;
         STRPAD H5T_STR_NULLTERM;
         CSET H5T_CSET_ASCII;
         CTYPE H5T_C_S1;
      }
      DATASPACE  SCALAR
   }
   ATTRIBUTE "pulseNumber" {
      DATATYPE  H5T_STD_I32LE
      DATASPACE  SIMPLE { ( 1 ) / ( 1 ) }
   }
   DATATYPE "coords" H5T_COMPOUND {
      H5T_IEEE_F64LE "R";
      H5T_IEEE_F64LE "Z";
   }
   DATASET "equilibriumStatus" {
      DATATYPE  "/equilibriumStatusType"
      DATASPACE  SIMPLE { ( 22845 ) / ( 22845 ) }
      STORAGE_LAYOUT {
</pre>
</div>
</div>


<div id="outline-container-orgbb01200" class="outline-3">
<h3 id="orgbb01200">Extracting data from a dataset with <code>h5dump</code></h3>
<div class="outline-text-3" id="text-orgbb01200">
<p>
For a full list of <code>h5dump</code> options run <code>h5dump -h</code>.
The <code>h5dump</code> command to extract a 10 by 10 block (<code>-s "0,0'" -k "10,10"</code>) of
elements from a two-dimensional dataset is shown below:
</p>

<div class="org-src-container">
<pre class="src src-shell">h5dump --filedriver=hdfs <span style="color: #008000;">\</span>
       --hdfs-attrs=<span style="color: #008000;">"(hdfs://jelly.ad.hdfgroup.org,8020,,,)"</span> <span style="color: #008000;">\</span>
       -d /output/fluxFunctionProfiles/poloidalFluxArea <span style="color: #008000;">\</span>
       -s <span style="color: #008000;">"0,0"</span> -k <span style="color: #008000;">"10,10"</span> <span style="color: #008000;">\</span>
    /tmp/efitOut.nc
</pre>
</div>

<pre class="example">

&gt; &gt; &gt; HDF5 "/tmp/efitOut.nc" {
DATASET "/output/fluxFunctionProfiles/poloidalFluxArea" {
   DATATYPE  H5T_IEEE_F64LE
   DATASPACE  SIMPLE { ( 22845, 33 ) / ( 22845, 33 ) }
   SUBSET {
      START ( 0, 0 );
      STRIDE ( 1, 1 );
      COUNT ( 1, 1 );
      BLOCK ( 10, 10 );
      DATA {
      (0,0): 0, 0.105177, 0.211194, 0.317949, 0.425508, 0.53394, 0.643315,
      (0,7): 0.753705, 0.865191, 0.97786,
      (1,0): 0, 0.102888, 0.206772, 0.311557, 0.417309, 0.524091, 0.631973,
      (1,7): 0.741024, 0.851326, 0.962959,
      (2,0): 0, 0.104089, 0.209173, 0.315166, 0.422121, 0.53011, 0.639202,
      (2,7): 0.749467, 0.860991, 0.973854,
      (3,0): 0, 0.10498, 0.210948, 0.317801, 0.425605, 0.534427, 0.644339,
      (3,7): 0.755412, 0.867731, 0.981378,
      (4,0): 0, 0.10605, 0.213064, 0.320959, 0.429793, 0.539632, 0.65055,
      (4,7): 0.76262, 0.875928, 0.990556,
      (5,0): 0, 0.108181, 0.217261, 0.327153, 0.437919, 0.549636, 0.66237,
      (5,7): 0.776199, 0.891211, 1.00749,
      (6,0): 0, 0.107151, 0.215293, 0.324331, 0.43433, 0.545359, 0.65749,
      (6,7): 0.7708, 0.885371, 1.00129,
      (7,0): 0, 0.110211, 0.22127, 0.33308, 0.445711, 0.559235, 0.673728,
      (7,7): 0.789269, 0.905944, 1.02384,
      (8,0): 0, 0.112895, 0.226538, 0.340842, 0.455874, 0.571715, 0.688438,
      (8,7): 0.806128, 0.924873, 1.04476,
      (9,0): 0, 0.112122, 0.225054, 0.338708, 0.453155, 0.568472, 0.68473,
      (9,7): 0.802015, 0.920413, 1.04001
      }
   }
   ATTRIBUTE "DIMENSION_LIST" {
      DATATYPE  H5T_VLEN { H5T_REFERENCE { H5T_STD_REF_OBJECT }}
      DATASPACE  SIMPLE { ( 2 ) / ( 2 ) }
      DATA {
      (0): (DATASET 9769 /time ),
      (1): (DATASET 240402 /output/fluxFunctionProfiles/normalizedPoloidalFlux )
      }
   }
   ATTRIBUTE "_FillValue" {
      DATATYPE  H5T_IEEE_F64LE
      DATASPACE  SIMPLE { ( 1 ) / ( 1 ) }
      DATA {
      (0): nan
      }
   }
   ATTRIBUTE "title" {
      DATATYPE  H5T_STRING {
         STRSIZE 16;
         STRPAD H5T_STR_NULLTERM;
         CSET H5T_CSET_ASCII;
         CTYPE H5T_C_S1;
      }
      DATASPACE  SCALAR
      DATA {
      (0): "poloidalFluxArea"
      }
   }
   ATTRIBUTE "units" {
      DATATYPE  H5T_STRING {
         STRSIZE 3;
         STRPAD H5T_STR_NULLTERM;
         CSET H5T_CSET_ASCII;
         CTYPE H5T_C_S1;
      }
      DATASPACE  SCALAR
      DATA {
      (0): "m^2"
      }
   }
}
}
</pre>
</div>
</div>


<div id="outline-container-org7631938" class="outline-3">
<h3 id="org7631938">Use Hadoop Streaming to collect data from multiple HDF5 files</h3>
<div class="outline-text-3" id="text-org7631938">
<p>
For more information on Hadoop streaming see the <a href="https://hadoop.apache.org/docs/r1.2.1/streaming.html">official documentation</a>.
Hadoop streaming is a utility that comes with the Hadoop distribution.
The utility allows you to create and run Map/Reduce jobs with any executable
or script as the mapper and/or the reducer. For example:
</p>

<pre class="example">
$HADOOP_HOME/bin/hadoop  jar $HADOOP_HOME/hadoop-streaming.jar \
    -input myInputDirs \
    -output myOutputDir \
    -mapper /bin/cat \
    -reducer /bin/wc
</pre>

<p>
We have created a simple example of a mapper and reducer written in C
which together determine the number of HDF5 objects (groups,
datasets, datatypes) in each file in a collection of HDF5 (and NetCDF-4)
files. The source code can be obtained from <a href="https://github.com/HDFGroup/hdf5vfd4hdfs-demo">GitHub</a>.
</p>

<p>
The HDF5 files to be examined are listed in two text files, <code>input1</code> and
<code>input2</code>. (The reason we use two files is to create more than one split
to be processed. The input text files are just too small for Hadoop to
create more than one split on its own.)
</p>

<div class="org-src-container">
<pre class="src src-shell">$<span style="color: #BA36A5;">HADOOP_HOME</span>/bin/hdfs dfs -cat hdfs://jelly.ad.hdfgroup.org:8020/tmp/input*
</pre>
</div>

<pre class="example">
/tmp/GSSTF_NCEP.3.1987.12.07.he5
/tmp/GSSTF_NCEP.3.1987.12.08.he5
/tmp/GSSTF_NCEP.3.1987.12.09.he5
/tmp/GSSTF_NCEP.3.1987.12.10.he5
/tmp/GSSTF_NCEP.3.1987.12.11.he5
/tmp/GSSTF_NCEP.3.1987.12.12.he5
/tmp/GSSTF_NCEP.3.1987.12.13.he5
/tmp/GSSTF_NCEP.3.1987.12.14.he5
/tmp/GSSTF_NCEP.3.1987.12.15.he5
/tmp/GSSTF_NCEP.3.1987.12.16.he5
/tmp/GSSTF_NCEP.3.1987.12.17.he5
/tmp/GSSTF_NCEP.3.1987.12.18.he5
/tmp/GSSTF_NCEP.3.1987.12.19.he5
/tmp/GSSTF_NCEP.3.1987.12.20.he5
/tmp/GSSTF_NCEP.3.1987.12.21.he5
/tmp/GSSTF_NCEP.3.1987.12.22.he5
/tmp/GSSTF_NCEP.3.1987.12.23.he5
/tmp/GSSTF_NCEP.3.1987.12.24.he5
/tmp/GSSTF_NCEP.3.1987.12.25.he5
/tmp/GSSTF_NCEP.3.1987.12.26.he5
/tmp/GSSTF_NCEP.3.1987.12.27.he5
/tmp/GSSTF_NCEP.3.1987.12.28.he5
/tmp/GSSTF_NCEP.3.1987.12.29.he5
/tmp/GSSTF_NCEP.3.1987.12.30.he5
/tmp/GSSTF_NCEP.3.1987.12.31.he5
/tmp/foo.h5
/tmp/sample.h5
/tmp/t.h5
/tmp/efitOut.nc
/tmp/GSSTF_NCEP.3.1987.12.01.he5
/tmp/GSSTF_NCEP.3.1987.12.02.he5
/tmp/GSSTF_NCEP.3.1987.12.03.he5
/tmp/GSSTF_NCEP.3.1987.12.04.he5
/tmp/GSSTF_NCEP.3.1987.12.05.he5
/tmp/GSSTF_NCEP.3.1987.12.06.he5
</pre>

<p>
The mapper, implemented in <code>hdfs-vfd-mapper.c</code> and wrapped in <code>mapper.sh</code>,
generates key-value pairs of the form
</p>

<pre class="example">
&lt;FILENAME&gt; [G,D,T]
...
</pre>

<p>
where the codes represent groups (<code>G</code>), datasets (<code>D</code>), or datatypes (<code>T</code>).
The reducer, implemented in <code>hdfs-vfd-reducer.c</code>, just counts the
number of codes in each category and presents the final result as
records of the form
</p>

<pre class="example">
&lt;FILENAME&gt; G #G D #D T #T
...
</pre>

<p>
Hadoop streaming can be invoked as follows:
</p>

<div class="org-src-container">
<pre class="src src-shell"><span style="color: #BA36A5;">HDFS_DIR</span>=hdfs://jelly.ad.hdfgroup.org:8020/tmp
<span style="color: #BA36A5;">INPUT1</span>=$<span style="color: #BA36A5;">HDFS_DIR</span>/input1
<span style="color: #BA36A5;">INPUT2</span>=$<span style="color: #BA36A5;">HDFS_DIR</span>/input2
<span style="color: #BA36A5;">OUTPUT</span>=$<span style="color: #BA36A5;">HDFS_DIR</span>/hdfs-vfd-output

<span style="color: #BA36A5;">MAPPER</span>=./mapper.sh
<span style="color: #BA36A5;">REDUCER</span>=./hdfs-vfd-reducer
<span style="color: #BA36A5;">MAPTASKS</span>=2
<span style="color: #BA36A5;">REDTASKS</span>=3

<span style="color: #8D8D84;"># </span><span style="color: #8D8D84; font-style: italic;">Delete output from previous runs</span>
$<span style="color: #BA36A5;">HADOOP_HOME</span>/bin/hdfs dfs -rm $<span style="color: #BA36A5;">OUTPUT</span>/*
$<span style="color: #BA36A5;">HADOOP_HOME</span>/bin/hdfs dfs -rmdir $<span style="color: #BA36A5;">OUTPUT</span>

$<span style="color: #BA36A5;">HADOOP_HOME</span>/bin/hadoop jar <span style="color: #008000;">\</span>
  $<span style="color: #BA36A5;">HADOOP_HOME</span>/share/hadoop/tools/lib/hadoop-*streaming*.jar <span style="color: #008000;">\</span>
  -D mapred.map.tasks=$<span style="color: #BA36A5;">MAPTASKS</span>  <span style="color: #008000;">\</span>
  -D mapred.reduce.tasks=$<span style="color: #BA36A5;">REDTASKS</span> <span style="color: #008000;">\</span>
  -input $<span style="color: #BA36A5;">INPUT1</span>  -input $<span style="color: #BA36A5;">INPUT2</span> <span style="color: #008000;">\</span>
  -output $<span style="color: #BA36A5;">OUTPUT</span> <span style="color: #008000;">\</span>
  -mapper $<span style="color: #BA36A5;">MAPPER</span> <span style="color: #008000;">\</span>
  -reducer $<span style="color: #BA36A5;">REDUCER</span>

$<span style="color: #BA36A5;">HADOOP_HOME</span>/bin/hdfs dfs -cat $<span style="color: #BA36A5;">OUTPUT</span>/part-*
</pre>
</div>

<pre class="example">

[gheber@jelly ESE]$ [gheber@jelly ESE]$ [gheber@jelly ESE]$ [gheber@jelly ESE]$ [gheber@jelly ESE]$ [gheber@jelly ESE]$ [gheber@jelly ESE]$ [gheber@jelly ESE]$ [gheber@jelly ESE]$ [gheber@jelly ESE]$ Deleted hdfs://jelly.ad.hdfgroup.org:8020/tmp/hdfs-vfd-output/_SUCCESS
Deleted hdfs://jelly.ad.hdfgroup.org:8020/tmp/hdfs-vfd-output/part-00000
Deleted hdfs://jelly.ad.hdfgroup.org:8020/tmp/hdfs-vfd-output/part-00001
Deleted hdfs://jelly.ad.hdfgroup.org:8020/tmp/hdfs-vfd-output/part-00002
[gheber@jelly ESE]$ [gheber@jelly ESE]$ &gt; &gt; &gt; &gt; &gt; &gt; &gt; 2018-10-25 10:28:31,885 INFO impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2018-10-25 10:28:31,935 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2018-10-25 10:28:31,935 INFO impl.MetricsSystemImpl: JobTracker metrics system started
2018-10-25 10:28:31,948 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2018-10-25 10:28:32,508 INFO mapred.FileInputFormat: Total input files to process : 2
2018-10-25 10:28:32,542 INFO mapreduce.JobSubmitter: number of splits:2
2018-10-25 10:28:32,562 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
2018-10-25 10:28:32,563 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps
2018-10-25 10:28:32,628 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local970602474_0001
2018-10-25 10:28:32,629 INFO mapreduce.JobSubmitter: Executing with tokens: []
2018-10-25 10:28:32,708 INFO mapreduce.Job: The url to track the job: http://localhost:8080/
2018-10-25 10:28:32,709 INFO mapreduce.Job: Running job: job_local970602474_0001
2018-10-25 10:28:32,711 INFO mapred.LocalJobRunner: OutputCommitter set in config null
2018-10-25 10:28:32,713 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter
2018-10-25 10:28:32,719 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2
2018-10-25 10:28:32,719 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2018-10-25 10:28:32,836 INFO mapred.LocalJobRunner: Waiting for map tasks
2018-10-25 10:28:32,840 INFO mapred.LocalJobRunner: Starting task: attempt_local970602474_0001_m_000000_0
2018-10-25 10:28:32,872 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2
2018-10-25 10:28:32,872 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2018-10-25 10:28:32,888 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
2018-10-25 10:28:32,898 INFO mapred.MapTask: Processing split: hdfs://jelly.ad.hdfgroup.org:8020/tmp/input1:0+825
2018-10-25 10:28:32,917 INFO mapred.MapTask: numReduceTasks: 3
2018-10-25 10:28:32,956 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)
2018-10-25 10:28:32,956 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100
2018-10-25 10:28:32,956 INFO mapred.MapTask: soft limit at 83886080
2018-10-25 10:28:32,956 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600
2018-10-25 10:28:32,956 INFO mapred.MapTask: kvstart = 26214396; length = 6553600
MapOutputBuffer
2018-10-25 10:28:32,965 INFO streaming.PipeMapRed: PipeMapRed exec [/mnt/wrk/gheber/Bitbucket/ghorg/ESE/././mapper.sh]
2018-10-25 10:28:32,970 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir
2018-10-25 10:28:32,970 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start
2018-10-25 10:28:32,971 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
2018-10-25 10:28:32,971 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
2018-10-25 10:28:32,972 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2018-10-25 10:28:32,972 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir
2018-10-25 10:28:32,972 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file
2018-10-25 10:28:32,972 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
2018-10-25 10:28:32,972 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length
2018-10-25 10:28:32,973 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id
2018-10-25 10:28:32,973 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name
2018-10-25 10:28:32,973 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
/usr/bin/bash: ml: line 1: syntax error: unexpected end of file
/usr/bin/bash: error importing function definition for `BASH_FUNC_ml'
/usr/bin/bash: module: line 1: syntax error: unexpected end of file
/usr/bin/bash: error importing function definition for `BASH_FUNC_module'
2018-10-25 10:28:33,057 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]
2018-10-25 10:28:33,058 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]
2018-10-25 10:28:33,713 INFO mapreduce.Job: Job job_local970602474_0001 running in uber mode : false
reduce 0%
2018-10-25 10:28:36,212 INFO streaming.PipeMapRed: Records R/W=25/1
2018-10-25 10:28:38,643 INFO streaming.PipeMapRed: MRErrorThread done
2018-10-25 10:28:38,644 INFO streaming.PipeMapRed: mapRedFinished
2018-10-25 10:28:38,648 INFO mapred.LocalJobRunner:
2018-10-25 10:28:38,648 INFO mapred.MapTask: Starting flush of map output
2018-10-25 10:28:38,648 INFO mapred.MapTask: Spilling map output
2018-10-25 10:28:38,648 INFO mapred.MapTask: bufstart = 0; bufend = 11375; bufvoid = 104857600
2018-10-25 10:28:38,648 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26213100(104852400); length = 1297/6553600
2018-10-25 10:28:38,664 INFO mapred.MapTask: Finished spill 0
2018-10-25 10:28:38,678 INFO mapred.Task: Task:attempt_local970602474_0001_m_000000_0 is done. And is in the process of committing
2018-10-25 10:28:38,683 INFO mapred.LocalJobRunner: Records R/W=25/1
2018-10-25 10:28:38,683 INFO mapred.Task: Task 'attempt_local970602474_0001_m_000000_0' done.
2018-10-25 10:28:38,692 INFO mapred.Task: Final Counters for attempt_local970602474_0001_m_000000_0: Counters: 22
        File System Counters
                FILE: Number of bytes read=176593
                FILE: Number of bytes written=688995
                FILE: Number of read operations=0
                FILE: Number of large read operations=0
                FILE: Number of write operations=0
                HDFS: Number of bytes read=825
                HDFS: Number of bytes written=0
                HDFS: Number of read operations=7
                HDFS: Number of large read operations=0
                HDFS: Number of write operations=1
        Map-Reduce Framework
                Map input records=25
                Map output records=325
                Map output bytes=11375
                Map output materialized bytes=12043
                Input split bytes=96
                Combine input records=0
                Spilled Records=325
                Failed Shuffles=0
                Merged Map outputs=0
                GC time elapsed (ms)=0
                Total committed heap usage (bytes)=1547698176
        File Input Format Counters
                Bytes Read=825
2018-10-25 10:28:38,692 INFO mapred.LocalJobRunner: Finishing task: attempt_local970602474_0001_m_000000_0
2018-10-25 10:28:38,693 INFO mapred.LocalJobRunner: Starting task: attempt_local970602474_0001_m_000001_0
2018-10-25 10:28:38,694 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2
2018-10-25 10:28:38,694 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2018-10-25 10:28:38,695 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
2018-10-25 10:28:38,696 INFO mapred.MapTask: Processing split: hdfs://jelly.ad.hdfgroup.org:8020/tmp/input2:0+251
2018-10-25 10:28:38,699 INFO mapred.MapTask: numReduceTasks: 3
reduce 0%
2018-10-25 10:28:38,734 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)
2018-10-25 10:28:38,734 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100
2018-10-25 10:28:38,734 INFO mapred.MapTask: soft limit at 83886080
2018-10-25 10:28:38,734 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600
2018-10-25 10:28:38,734 INFO mapred.MapTask: kvstart = 26214396; length = 6553600
MapOutputBuffer
2018-10-25 10:28:38,740 INFO streaming.PipeMapRed: PipeMapRed exec [/mnt/wrk/gheber/Bitbucket/ghorg/ESE/././mapper.sh]
/usr/bin/bash: ml: line 1: syntax error: unexpected end of file
/usr/bin/bash: error importing function definition for `BASH_FUNC_ml'
2018-10-25 10:28:38,750 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]
2018-10-25 10:28:38,750 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]
/usr/bin/bash: module: line 1: syntax error: unexpected end of file
/usr/bin/bash: error importing function definition for `BASH_FUNC_module'
2018-10-25 10:28:41,126 INFO streaming.PipeMapRed: Records R/W=10/1
2018-10-25 10:28:42,275 INFO streaming.PipeMapRed: MRErrorThread done
2018-10-25 10:28:42,276 INFO streaming.PipeMapRed: mapRedFinished
2018-10-25 10:28:42,277 INFO mapred.LocalJobRunner:
2018-10-25 10:28:42,277 INFO mapred.MapTask: Starting flush of map output
2018-10-25 10:28:42,277 INFO mapred.MapTask: Spilling map output
2018-10-25 10:28:42,277 INFO mapred.MapTask: bufstart = 0; bufend = 9096; bufvoid = 104857600
2018-10-25 10:28:42,277 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26212668(104850672); length = 1729/6553600
2018-10-25 10:28:42,281 INFO mapred.MapTask: Finished spill 0
2018-10-25 10:28:42,283 INFO mapred.Task: Task:attempt_local970602474_0001_m_000001_0 is done. And is in the process of committing
2018-10-25 10:28:42,287 INFO mapred.LocalJobRunner: Records R/W=10/1
2018-10-25 10:28:42,287 INFO mapred.Task: Task 'attempt_local970602474_0001_m_000001_0' done.
2018-10-25 10:28:42,288 INFO mapred.Task: Final Counters for attempt_local970602474_0001_m_000001_0: Counters: 22
        File System Counters
                FILE: Number of bytes read=176804
                FILE: Number of bytes written=699055
                FILE: Number of read operations=0
                FILE: Number of large read operations=0
                FILE: Number of write operations=0
                HDFS: Number of bytes read=1076
                HDFS: Number of bytes written=0
                HDFS: Number of read operations=9
                HDFS: Number of large read operations=0
                HDFS: Number of write operations=1
        Map-Reduce Framework
                Map input records=10
                Map output records=433
                Map output bytes=9096
                Map output materialized bytes=9980
                Input split bytes=96
                Combine input records=0
                Spilled Records=433
                Failed Shuffles=0
                Merged Map outputs=0
                GC time elapsed (ms)=0
                Total committed heap usage (bytes)=1547698176
        File Input Format Counters
                Bytes Read=251
2018-10-25 10:28:42,288 INFO mapred.LocalJobRunner: Finishing task: attempt_local970602474_0001_m_000001_0
2018-10-25 10:28:42,288 INFO mapred.LocalJobRunner: map task executor complete.
2018-10-25 10:28:42,295 INFO mapred.LocalJobRunner: Waiting for reduce tasks
2018-10-25 10:28:42,296 INFO mapred.LocalJobRunner: Starting task: attempt_local970602474_0001_r_000000_0
2018-10-25 10:28:42,307 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2
2018-10-25 10:28:42,307 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2018-10-25 10:28:42,308 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
2018-10-25 10:28:42,314 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@439c1391
2018-10-25 10:28:42,317 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2018-10-25 10:28:42,348 INFO reduce.MergeManagerImpl: The max number of bytes for a single in-memory shuffle cannot be larger than Integer.MAX_VALUE. Setting it to Integer.MAX_VALUE
2018-10-25 10:28:42,348 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=20041957376, maxSingleShuffleLimit=2147483647, mergeThreshold=13227692032, ioSortFactor=10, memToMemMergeOutputsThreshold=10
2018-10-25 10:28:42,352 INFO reduce.EventFetcher: attempt_local970602474_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
1 about to shuffle output of map attempt_local970602474_0001_m_000000_0 decomp: 3850 len: 3854 to MEMORY
2018-10-25 10:28:42,391 INFO reduce.InMemoryMapOutput: Read 3850 bytes from map-output for attempt_local970602474_0001_m_000000_0
map-output of size: 3850, inMemoryMapOutputs.size() -&gt; 1, commitMemory -&gt; 0, usedMemory -&gt;3850
1 about to shuffle output of map attempt_local970602474_0001_m_000001_0 decomp: 964 len: 968 to MEMORY
2018-10-25 10:28:42,396 INFO reduce.InMemoryMapOutput: Read 964 bytes from map-output for attempt_local970602474_0001_m_000001_0
map-output of size: 964, inMemoryMapOutputs.size() -&gt; 2, commitMemory -&gt; 3850, usedMemory -&gt;4814
2018-10-25 10:28:42,397 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning
2018-10-25 10:28:42,398 INFO mapred.LocalJobRunner: 2 / 2 copied.
2018-10-25 10:28:42,398 INFO reduce.MergeManagerImpl: finalMerge called with 2 in-memory map-outputs and 0 on-disk map-outputs
2018-10-25 10:28:42,406 INFO mapred.Merger: Merging 2 sorted segments
2018-10-25 10:28:42,406 INFO mapred.Merger: Down to the last merge-pass, with 2 segments left of total size: 4744 bytes
2018-10-25 10:28:42,409 INFO reduce.MergeManagerImpl: Merged 2 segments, 4814 bytes to disk to satisfy reduce memory limit
2018-10-25 10:28:42,409 INFO reduce.MergeManagerImpl: Merging 1 files, 4816 bytes from disk
2018-10-25 10:28:42,410 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce
2018-10-25 10:28:42,410 INFO mapred.Merger: Merging 1 sorted segments
2018-10-25 10:28:42,411 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 4777 bytes
2018-10-25 10:28:42,411 INFO mapred.LocalJobRunner: 2 / 2 copied.
2018-10-25 10:28:42,415 INFO streaming.PipeMapRed: PipeMapRed exec [/mnt/wrk/gheber/Bitbucket/ghorg/ESE/././hdfs-vfd-reducer]
2018-10-25 10:28:42,417 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address
2018-10-25 10:28:42,418 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps
2018-10-25 10:28:42,548 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]
2018-10-25 10:28:42,549 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]
2018-10-25 10:28:42,551 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]
2018-10-25 10:28:42,554 INFO streaming.PipeMapRed: MRErrorThread done
2018-10-25 10:28:42,557 INFO streaming.PipeMapRed: Records R/W=130/1
2018-10-25 10:28:42,557 INFO streaming.PipeMapRed: mapRedFinished
2018-10-25 10:28:42,660 INFO mapred.Task: Task:attempt_local970602474_0001_r_000000_0 is done. And is in the process of committing
2018-10-25 10:28:42,663 INFO mapred.LocalJobRunner: 2 / 2 copied.
2018-10-25 10:28:42,663 INFO mapred.Task: Task attempt_local970602474_0001_r_000000_0 is allowed to commit now
2018-10-25 10:28:42,700 INFO output.FileOutputCommitter: Saved output of task 'attempt_local970602474_0001_r_000000_0' to hdfs://jelly.ad.hdfgroup.org:8020/tmp/hdfs-vfd-output
reduce
2018-10-25 10:28:42,701 INFO mapred.Task: Task 'attempt_local970602474_0001_r_000000_0' done.
2018-10-25 10:28:42,702 INFO mapred.Task: Final Counters for attempt_local970602474_0001_r_000000_0: Counters: 29
        File System Counters
                FILE: Number of bytes read=189972
                FILE: Number of bytes written=703871
                FILE: Number of read operations=0
                FILE: Number of large read operations=0
                FILE: Number of write operations=0
                HDFS: Number of bytes read=1076
                HDFS: Number of bytes written=452
                HDFS: Number of read operations=14
                HDFS: Number of large read operations=0
                HDFS: Number of write operations=3
        Map-Reduce Framework
                Combine input records=0
                Combine output records=0
                Reduce input groups=10
                Reduce shuffle bytes=4822
                Reduce input records=130
                Reduce output records=10
                Spilled Records=130
                Shuffled Maps =2
                Failed Shuffles=0
                Merged Map outputs=2
                GC time elapsed (ms)=0
                Total committed heap usage (bytes)=1547698176
        Shuffle Errors
                BAD_ID=0
                CONNECTION=0
                IO_ERROR=0
                WRONG_LENGTH=0
                WRONG_MAP=0
                WRONG_REDUCE=0
        File Output Format Counters
                Bytes Written=452
2018-10-25 10:28:42,702 INFO mapred.LocalJobRunner: Finishing task: attempt_local970602474_0001_r_000000_0
2018-10-25 10:28:42,703 INFO mapred.LocalJobRunner: Starting task: attempt_local970602474_0001_r_000001_0
2018-10-25 10:28:42,705 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2
2018-10-25 10:28:42,705 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2018-10-25 10:28:42,705 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
2018-10-25 10:28:42,705 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@578316ce
2018-10-25 10:28:42,706 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2018-10-25 10:28:42,707 INFO reduce.MergeManagerImpl: The max number of bytes for a single in-memory shuffle cannot be larger than Integer.MAX_VALUE. Setting it to Integer.MAX_VALUE
2018-10-25 10:28:42,707 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=20041957376, maxSingleShuffleLimit=2147483647, mergeThreshold=13227692032, ioSortFactor=10, memToMemMergeOutputsThreshold=10
2018-10-25 10:28:42,708 INFO reduce.EventFetcher: attempt_local970602474_0001_r_000001_0 Thread started: EventFetcher for fetching Map Completion Events
2 about to shuffle output of map attempt_local970602474_0001_m_000000_0 decomp: 3850 len: 3854 to MEMORY
2018-10-25 10:28:42,713 INFO reduce.InMemoryMapOutput: Read 3850 bytes from map-output for attempt_local970602474_0001_m_000000_0
map-output of size: 3850, inMemoryMapOutputs.size() -&gt; 1, commitMemory -&gt; 0, usedMemory -&gt;3850
2 about to shuffle output of map attempt_local970602474_0001_m_000001_0 decomp: 8008 len: 8012 to MEMORY
2018-10-25 10:28:42,716 INFO reduce.InMemoryMapOutput: Read 8008 bytes from map-output for attempt_local970602474_0001_m_000001_0
map-output of size: 8008, inMemoryMapOutputs.size() -&gt; 2, commitMemory -&gt; 3850, usedMemory -&gt;11858
2018-10-25 10:28:42,717 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning
2018-10-25 10:28:42,717 INFO mapred.LocalJobRunner: 2 / 2 copied.
2018-10-25 10:28:42,717 INFO reduce.MergeManagerImpl: finalMerge called with 2 in-memory map-outputs and 0 on-disk map-outputs
2018-10-25 10:28:42,719 INFO mapred.Merger: Merging 2 sorted segments
2018-10-25 10:28:42,719 INFO mapred.Merger: Down to the last merge-pass, with 2 segments left of total size: 11788 bytes
2018-10-25 10:28:42,721 INFO reduce.MergeManagerImpl: Merged 2 segments, 11858 bytes to disk to satisfy reduce memory limit
2018-10-25 10:28:42,722 INFO reduce.MergeManagerImpl: Merging 1 files, 11860 bytes from disk
2018-10-25 10:28:42,722 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce
2018-10-25 10:28:42,722 INFO mapred.Merger: Merging 1 sorted segments
2018-10-25 10:28:42,722 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 11821 bytes
2018-10-25 10:28:42,722 INFO mapred.LocalJobRunner: 2 / 2 copied.
2018-10-25 10:28:42,726 INFO streaming.PipeMapRed: PipeMapRed exec [/mnt/wrk/gheber/Bitbucket/ghorg/ESE/././hdfs-vfd-reducer]
reduce 33%
2018-10-25 10:28:42,766 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]
2018-10-25 10:28:42,766 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]
2018-10-25 10:28:42,767 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]
2018-10-25 10:28:42,773 INFO streaming.PipeMapRed: MRErrorThread done
2018-10-25 10:28:42,774 INFO streaming.PipeMapRed: Records R/W=483/1
2018-10-25 10:28:42,775 INFO streaming.PipeMapRed: mapRedFinished
2018-10-25 10:28:42,832 INFO mapred.Task: Task:attempt_local970602474_0001_r_000001_0 is done. And is in the process of committing
2018-10-25 10:28:42,835 INFO mapred.LocalJobRunner: 2 / 2 copied.
2018-10-25 10:28:42,835 INFO mapred.Task: Task attempt_local970602474_0001_r_000001_0 is allowed to commit now
2018-10-25 10:28:42,857 INFO output.FileOutputCommitter: Saved output of task 'attempt_local970602474_0001_r_000001_0' to hdfs://jelly.ad.hdfgroup.org:8020/tmp/hdfs-vfd-output
reduce
2018-10-25 10:28:42,858 INFO mapred.Task: Task 'attempt_local970602474_0001_r_000001_0' done.
2018-10-25 10:28:42,859 INFO mapred.Task: Final Counters for attempt_local970602474_0001_r_000001_0: Counters: 29
        File System Counters
                FILE: Number of bytes read=215100
                FILE: Number of bytes written=715731
                FILE: Number of read operations=0
                FILE: Number of large read operations=0
                FILE: Number of write operations=0
                HDFS: Number of bytes read=1076
                HDFS: Number of bytes written=984
                HDFS: Number of read operations=19
                HDFS: Number of large read operations=0
                HDFS: Number of write operations=5
        Map-Reduce Framework
                Combine input records=0
                Combine output records=0
                Reduce input groups=13
                Reduce shuffle bytes=11866
                Reduce input records=483
                Reduce output records=13
                Spilled Records=483
                Shuffled Maps =2
                Failed Shuffles=0
                Merged Map outputs=2
                GC time elapsed (ms)=0
                Total committed heap usage (bytes)=1547698176
        Shuffle Errors
                BAD_ID=0
                CONNECTION=0
                IO_ERROR=0
                WRONG_LENGTH=0
                WRONG_MAP=0
                WRONG_REDUCE=0
        File Output Format Counters
                Bytes Written=532
2018-10-25 10:28:42,859 INFO mapred.LocalJobRunner: Finishing task: attempt_local970602474_0001_r_000001_0
2018-10-25 10:28:42,859 INFO mapred.LocalJobRunner: Starting task: attempt_local970602474_0001_r_000002_0
2018-10-25 10:28:42,861 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2
2018-10-25 10:28:42,861 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2018-10-25 10:28:42,862 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
2018-10-25 10:28:42,862 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@6aff7da3
2018-10-25 10:28:42,862 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2018-10-25 10:28:42,863 INFO reduce.MergeManagerImpl: The max number of bytes for a single in-memory shuffle cannot be larger than Integer.MAX_VALUE. Setting it to Integer.MAX_VALUE
2018-10-25 10:28:42,863 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=20041957376, maxSingleShuffleLimit=2147483647, mergeThreshold=13227692032, ioSortFactor=10, memToMemMergeOutputsThreshold=10
2018-10-25 10:28:42,864 INFO reduce.EventFetcher: attempt_local970602474_0001_r_000002_0 Thread started: EventFetcher for fetching Map Completion Events
3 about to shuffle output of map attempt_local970602474_0001_m_000000_0 decomp: 4331 len: 4335 to MEMORY
2018-10-25 10:28:42,886 INFO reduce.InMemoryMapOutput: Read 4331 bytes from map-output for attempt_local970602474_0001_m_000000_0
map-output of size: 4331, inMemoryMapOutputs.size() -&gt; 1, commitMemory -&gt; 0, usedMemory -&gt;4331
3 about to shuffle output of map attempt_local970602474_0001_m_000001_0 decomp: 996 len: 1000 to MEMORY
2018-10-25 10:28:42,891 INFO reduce.InMemoryMapOutput: Read 996 bytes from map-output for attempt_local970602474_0001_m_000001_0
map-output of size: 996, inMemoryMapOutputs.size() -&gt; 2, commitMemory -&gt; 4331, usedMemory -&gt;5327
2018-10-25 10:28:42,892 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning
2018-10-25 10:28:42,893 INFO mapred.LocalJobRunner: 2 / 2 copied.
2018-10-25 10:28:42,894 INFO reduce.MergeManagerImpl: finalMerge called with 2 in-memory map-outputs and 0 on-disk map-outputs
2018-10-25 10:28:42,895 INFO mapred.Merger: Merging 2 sorted segments
2018-10-25 10:28:42,895 INFO mapred.Merger: Down to the last merge-pass, with 2 segments left of total size: 5257 bytes
2018-10-25 10:28:42,897 INFO reduce.MergeManagerImpl: Merged 2 segments, 5327 bytes to disk to satisfy reduce memory limit
2018-10-25 10:28:42,897 INFO reduce.MergeManagerImpl: Merging 1 files, 5329 bytes from disk
2018-10-25 10:28:42,898 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce
2018-10-25 10:28:42,898 INFO mapred.Merger: Merging 1 sorted segments
2018-10-25 10:28:42,898 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 5290 bytes
2018-10-25 10:28:42,899 INFO mapred.LocalJobRunner: 2 / 2 copied.
2018-10-25 10:28:42,903 INFO streaming.PipeMapRed: PipeMapRed exec [/mnt/wrk/gheber/Bitbucket/ghorg/ESE/././hdfs-vfd-reducer]
2018-10-25 10:28:42,934 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]
2018-10-25 10:28:42,934 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]
2018-10-25 10:28:42,934 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]
2018-10-25 10:28:42,936 INFO streaming.PipeMapRed: MRErrorThread done
2018-10-25 10:28:42,937 INFO streaming.PipeMapRed: Records R/W=145/1
2018-10-25 10:28:42,938 INFO streaming.PipeMapRed: mapRedFinished
2018-10-25 10:28:42,982 INFO mapred.Task: Task:attempt_local970602474_0001_r_000002_0 is done. And is in the process of committing
2018-10-25 10:28:42,985 INFO mapred.LocalJobRunner: 2 / 2 copied.
2018-10-25 10:28:42,985 INFO mapred.Task: Task attempt_local970602474_0001_r_000002_0 is allowed to commit now
2018-10-25 10:28:43,024 INFO output.FileOutputCommitter: Saved output of task 'attempt_local970602474_0001_r_000002_0' to hdfs://jelly.ad.hdfgroup.org:8020/tmp/hdfs-vfd-output
reduce
2018-10-25 10:28:43,025 INFO mapred.Task: Task 'attempt_local970602474_0001_r_000002_0' done.
2018-10-25 10:28:43,026 INFO mapred.Task: Final Counters for attempt_local970602474_0001_r_000002_0: Counters: 29
        File System Counters
                FILE: Number of bytes read=225924
                FILE: Number of bytes written=721060
                FILE: Number of read operations=0
                FILE: Number of large read operations=0
                FILE: Number of write operations=0
                HDFS: Number of bytes read=1076
                HDFS: Number of bytes written=1505
                HDFS: Number of read operations=24
                HDFS: Number of large read operations=0
                HDFS: Number of write operations=7
        Map-Reduce Framework
                Combine input records=0
                Combine output records=0
                Reduce input groups=12
                Reduce shuffle bytes=5335
                Reduce input records=145
                Reduce output records=12
                Spilled Records=145
                Shuffled Maps =2
                Failed Shuffles=0
                Merged Map outputs=2
                GC time elapsed (ms)=18
                Total committed heap usage (bytes)=1560805376
        Shuffle Errors
                BAD_ID=0
                CONNECTION=0
                IO_ERROR=0
                WRONG_LENGTH=0
                WRONG_MAP=0
                WRONG_REDUCE=0
        File Output Format Counters
                Bytes Written=521
2018-10-25 10:28:43,026 INFO mapred.LocalJobRunner: Finishing task: attempt_local970602474_0001_r_000002_0
2018-10-25 10:28:43,026 INFO mapred.LocalJobRunner: reduce task executor complete.
reduce 100%
2018-10-25 10:28:43,741 INFO mapreduce.Job: Job job_local970602474_0001 completed successfully
2018-10-25 10:28:43,769 INFO mapreduce.Job: Counters: 35
        File System Counters
                FILE: Number of bytes read=984393
                FILE: Number of bytes written=3528712
                FILE: Number of read operations=0
                FILE: Number of large read operations=0
                FILE: Number of write operations=0
                HDFS: Number of bytes read=5129
                HDFS: Number of bytes written=2941
                HDFS: Number of read operations=73
                HDFS: Number of large read operations=0
                HDFS: Number of write operations=17
        Map-Reduce Framework
                Map input records=35
                Map output records=758
                Map output bytes=20471
                Map output materialized bytes=22023
                Input split bytes=192
                Combine input records=0
                Combine output records=0
                Reduce input groups=35
                Reduce shuffle bytes=22023
                Reduce input records=758
                Reduce output records=35
                Spilled Records=1516
                Shuffled Maps =6
                Failed Shuffles=0
                Merged Map outputs=6
                GC time elapsed (ms)=18
                Total committed heap usage (bytes)=7751598080
        Shuffle Errors
                BAD_ID=0
                CONNECTION=0
                IO_ERROR=0
                WRONG_LENGTH=0
                WRONG_MAP=0
                WRONG_REDUCE=0
        File Input Format Counters
                Bytes Read=1076
        File Output Format Counters
                Bytes Written=1505
2018-10-25 10:28:43,770 INFO streaming.StreamJob: Output directory: hdfs://jelly.ad.hdfgroup.org:8020/tmp/hdfs-vfd-output
[gheber@jelly ESE]$ /tmp/GSSTF_NCEP.3.1987.12.02.he5	G 8	D 5	T 0
/tmp/GSSTF_NCEP.3.1987.12.05.he5	G 8	D 5	T 0
/tmp/GSSTF_NCEP.3.1987.12.08.he5	G 8	D 5	T 0
/tmp/GSSTF_NCEP.3.1987.12.11.he5	G 8	D 5	T 0
/tmp/GSSTF_NCEP.3.1987.12.14.he5	G 8	D 5	T 0
/tmp/GSSTF_NCEP.3.1987.12.17.he5	G 8	D 5	T 0
/tmp/GSSTF_NCEP.3.1987.12.20.he5	G 8	D 5	T 0
/tmp/GSSTF_NCEP.3.1987.12.23.he5	G 8	D 5	T 0
/tmp/GSSTF_NCEP.3.1987.12.26.he5	G 8	D 5	T 0
/tmp/GSSTF_NCEP.3.1987.12.29.he5	G 8	 D 5	 T 0
/tmp/GSSTF_NCEP.3.1987.12.03.he5	G 8	D 5	T 0
/tmp/GSSTF_NCEP.3.1987.12.06.he5	G 8	D 5	T 0
/tmp/GSSTF_NCEP.3.1987.12.09.he5	G 8	D 5	T 0
/tmp/GSSTF_NCEP.3.1987.12.12.he5	G 8	D 5	T 0
/tmp/GSSTF_NCEP.3.1987.12.15.he5	G 8	D 5	T 0
/tmp/GSSTF_NCEP.3.1987.12.18.he5	G 8	D 5	T 0
/tmp/GSSTF_NCEP.3.1987.12.21.he5	G 8	D 5	T 0
/tmp/GSSTF_NCEP.3.1987.12.24.he5	G 8	D 5	T 0
/tmp/GSSTF_NCEP.3.1987.12.27.he5	G 8	D 5	T 0
/tmp/GSSTF_NCEP.3.1987.12.30.he5	G 8	D 5	T 0
/tmp/efitOut.nc	G 35	D 305	T 7
/tmp/sample.h5	G 4	D 0	T 0
/tmp/t.h5	G 1	 D 1	 T 0
/tmp/GSSTF_NCEP.3.1987.12.01.he5	G 8	D 5	T 0
/tmp/GSSTF_NCEP.3.1987.12.04.he5	G 8	D 5	T 0
/tmp/GSSTF_NCEP.3.1987.12.07.he5	G 8	D 5	T 0
/tmp/GSSTF_NCEP.3.1987.12.10.he5	G 8	D 5	T 0
/tmp/GSSTF_NCEP.3.1987.12.13.he5	G 8	D 5	T 0
/tmp/GSSTF_NCEP.3.1987.12.16.he5	G 8	D 5	T 0
/tmp/GSSTF_NCEP.3.1987.12.19.he5	G 8	D 5	T 0
/tmp/GSSTF_NCEP.3.1987.12.22.he5	G 8	D 5	T 0
/tmp/GSSTF_NCEP.3.1987.12.25.he5	G 8	D 5	T 0
/tmp/GSSTF_NCEP.3.1987.12.28.he5	G 8	D 5	T 0
/tmp/GSSTF_NCEP.3.1987.12.31.he5	G 8	D 5	T 0
/tmp/foo.h5	G 2	 D 0	 T 0
</pre>
</div>
</div>
</div>


<div id="outline-container-orgce50a87" class="outline-2">
<h2 id="orgce50a87">Summary</h2>
<div class="outline-text-2" id="text-orgce50a87">
<p>
The HDF5 VFD for HDFS provides transparent <i>read access</i> to HDF5 files stored
in HDFS file systems. No code changes other than loading the HDFS VFD and
linking against an updated version of the HDF5 library are required,
This applies to the HDF5 command line tools as well as existing applications.
You can use this VFD to bulk process HDF5 (and NetCDF-4) files stored in
HDFS with frameworks such as Hadoop streaming.
</p>
</div>
</div>
</div>
</body>
</html>
